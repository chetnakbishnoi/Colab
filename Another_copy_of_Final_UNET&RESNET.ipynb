{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chetnakbishnoi/Colab/blob/main/Another_copy_of_Final_UNET%26RESNET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMS6DBzjC7u_",
        "outputId": "d6369a2a-9130-41ea-da88-f11e5794f65d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZhITMFBDpae",
        "outputId": "34d6107e-6deb-4f17-f432-5ae48730ae9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting geotile\n",
            "  Downloading geotile-1.1.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: gdal in /usr/local/lib/python3.10/dist-packages (from geotile) (3.6.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from geotile) (1.26.4)\n",
            "Collecting geopandas (from geotile)\n",
            "  Downloading geopandas-1.0.1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting rasterio (from geotile)\n",
            "  Downloading rasterio-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting pyogrio>=0.7.2 (from geopandas->geotile)\n",
            "  Downloading pyogrio-0.10.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from geopandas->geotile) (24.1)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from geopandas->geotile) (2.2.2)\n",
            "Collecting pyproj>=3.3.0 (from geopandas->geotile)\n",
            "  Downloading pyproj-3.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
            "Collecting shapely>=2.0.0 (from geopandas->geotile)\n",
            "  Downloading shapely-2.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting affine (from rasterio->geotile)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from rasterio->geotile) (24.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from rasterio->geotile) (2024.8.30)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.10/dist-packages (from rasterio->geotile) (8.1.7)\n",
            "Collecting cligj>=0.5 (from rasterio->geotile)\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting click-plugins (from rasterio->geotile)\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from rasterio->geotile) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.0->geopandas->geotile) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.0->geopandas->geotile) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.0->geopandas->geotile) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->geopandas->geotile) (1.16.0)\n",
            "Downloading geotile-1.1.0-py3-none-any.whl (12 kB)\n",
            "Downloading geopandas-1.0.1-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.6/323.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rasterio-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Downloading pyogrio-0.10.0-cp310-cp310-manylinux_2_28_x86_64.whl (23.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.9/23.9 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproj-3.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m112.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shapely-2.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Installing collected packages: shapely, pyproj, pyogrio, cligj, click-plugins, affine, rasterio, geopandas, geotile\n",
            "Successfully installed affine-2.4.0 click-plugins-1.1.1 cligj-0.7.2 geopandas-1.0.1 geotile-1.1.0 pyogrio-0.10.0 pyproj-3.7.0 rasterio-1.4.1 shapely-2.0.6\n"
          ]
        }
      ],
      "source": [
        "pip install geotile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Znqv2cHwDwfN",
        "outputId": "2b321595-a612-465f-f28f-801a41dd6486"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fiona\n",
            "  Downloading fiona-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (56 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/56.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.6/56.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from fiona) (24.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from fiona) (2024.8.30)\n",
            "Requirement already satisfied: click~=8.0 in /usr/local/lib/python3.10/dist-packages (from fiona) (8.1.7)\n",
            "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.10/dist-packages (from fiona) (1.1.1)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from fiona) (0.7.2)\n",
            "Downloading fiona-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fiona\n",
            "Successfully installed fiona-1.10.1\n"
          ]
        }
      ],
      "source": [
        "pip install fiona"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTmkw6A0Dz4k"
      },
      "outputs": [],
      "source": [
        "import fiona"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTo2gnIGv6uM"
      },
      "outputs": [],
      "source": [
        "from geotile import GeoTile\n",
        "gt = GeoTile('/content/drive/MyDrive/iirs/Delhi_NCR_Buildings.tif')\n",
        "gt.rasterization('/content/drive/MyDrive/utm_converted/utm_again.shp', out_path='/content/drive/MyDrive/iirs/Output_Delhi_NCR_Buildings.tif')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxCgLD68Gzl2"
      },
      "outputs": [],
      "source": [
        "gt_img = GeoTile('/content/drive/MyDrive/iirs/Delhi_NCR_Buildings.tif')\n",
        "gt_img.generate_tiles(output_folder='/content/drive/MyDrive/tiles_of_tiff_delhi_ncr')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFJYO7mnGzzV"
      },
      "outputs": [],
      "source": [
        "# create the tiles of the raster mask\n",
        "gt_mask = GeoTile('/content/drive/MyDrive/iirs/Output_Delhi_NCR_Buildings.tif')\n",
        "gt_mask.generate_tiles(output_folder='/content/drive/MyDrive/raster_mask_tiles')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTb56_VRNxsB",
        "outputId": "83143c4b-20fb-4cf0-f7f2-18a529b6d61f"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/tiles_of_tiff_delhi_ncr'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-872e73da084d>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Preprocess all satellite images and masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msatellite_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".tif\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# File paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/tiles_of_tiff_delhi_ncr'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from skimage import io\n",
        "from skimage.exposure import rescale_intensity\n",
        "\n",
        "# Directories for the satellite imagery and masks\n",
        "satellite_dir = '/content/drive/MyDrive/tiles_of_tiff_delhi_ncr'\n",
        "mask_dir = '/content/drive/MyDrive/raster_mask_tiles'\n",
        "output_sat_dir = '/content/drive/MyDrive/processed_tiles_of_tiff_delhi_ncr'\n",
        "output_mask_dir = '/content/drive/MyDrive/processed_raster_mask_tiles'\n",
        "\n",
        "# Function to preprocess satellite imagery\n",
        "def preprocess_satellite_image(image):\n",
        "    # Replace NaN values with zeros\n",
        "    image = np.nan_to_num(image, nan=0)\n",
        "\n",
        "    # Normalize to [0, 1] range\n",
        "    image = rescale_intensity(image, in_range=(image.min(), image.max()), out_range=(0, 1))\n",
        "\n",
        "    return image\n",
        "\n",
        "# Function to ensure mask is binary\n",
        "def preprocess_mask(mask):\n",
        "    # Clip values to ensure mask contains only 0 and 1\n",
        "    mask = np.clip(mask, 0, 1)\n",
        "\n",
        "    return mask\n",
        "\n",
        "# Preprocess all satellite images and masks\n",
        "for filename in os.listdir(satellite_dir):\n",
        "    if filename.endswith(\".tif\"):\n",
        "        # File paths\n",
        "        satellite_path = os.path.join(satellite_dir, filename)\n",
        "        mask_path = os.path.join(mask_dir, filename)  # Assuming the mask filenames match the satellite images\n",
        "\n",
        "        # Load the satellite image and corresponding mask\n",
        "        satellite_img = io.imread(satellite_path)\n",
        "        mask_img = io.imread(mask_path)\n",
        "\n",
        "        # Preprocess satellite image and mask\n",
        "        preprocessed_satellite = preprocess_satellite_image(satellite_img)\n",
        "        preprocessed_mask = preprocess_mask(mask_img)\n",
        "\n",
        "        # Save the preprocessed satellite image and mask\n",
        "        output_sat_path = os.path.join(output_sat_dir, filename)\n",
        "        output_mask_path = os.path.join(output_mask_dir, filename)\n",
        "        io.imsave(output_sat_path, (preprocessed_satellite * 255).astype(np.uint8))  # Save satellite image\n",
        "        io.imsave(output_mask_path, preprocessed_mask.astype(np.uint8))  # Save mask\n",
        "\n",
        "print(\"Preprocessing complete for all tiles.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sl8_PKvHi1Gt"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Conv2D, BatchNormalization, Activation, Add,\n",
        "    ZeroPadding2D, MaxPooling2D, AveragePooling2D, Dropout,\n",
        "    UpSampling2D\n",
        ")\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "\n",
        "# Define the ResNet50 architecture\n",
        "def identity_block(X, f, filters, stage, block, l2_reg=0.01):\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "    F1, F2, F3 = filters\n",
        "\n",
        "    X_shortcut = X\n",
        "\n",
        "    # First component of main path\n",
        "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(1, 1), padding='valid',\n",
        "               name=conv_name_base + '2a', kernel_initializer=glorot_uniform(seed=0),\n",
        "               kernel_regularizer=tf.keras.regularizers.l2(l2_reg))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Second component of main path\n",
        "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same',\n",
        "               name=conv_name_base + '2b', kernel_initializer=glorot_uniform(seed=0),\n",
        "               kernel_regularizer=tf.keras.regularizers.l2(l2_reg))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path\n",
        "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid',\n",
        "               name=conv_name_base + '2c', kernel_initializer=glorot_uniform(seed=0),\n",
        "               kernel_regularizer=tf.keras.regularizers.l2(l2_reg))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
        "\n",
        "    # Add shortcut value to main path\n",
        "    X = Add()([X_shortcut, X])\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    return X\n",
        "\n",
        "def convolutional_block(X, f, filters, stage, block, s=2, l2_reg=0.01):\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "    F1, F2, F3 = filters\n",
        "\n",
        "    X_shortcut = X\n",
        "\n",
        "    # First component of main path\n",
        "    X = Conv2D(F1, (1, 1), strides=(s, s), name=conv_name_base + '2a', padding='valid',\n",
        "               kernel_initializer=glorot_uniform(seed=0),\n",
        "               kernel_regularizer=tf.keras.regularizers.l2(l2_reg))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Second component of main path\n",
        "    X = Conv2D(F2, (f, f), strides=(1, 1), name=conv_name_base + '2b', padding='same',\n",
        "               kernel_initializer=glorot_uniform(seed=0),\n",
        "               kernel_regularizer=tf.keras.regularizers.l2(l2_reg))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path\n",
        "    X = Conv2D(F3, (1, 1), strides=(1, 1), name=conv_name_base + '2c', padding='valid',\n",
        "               kernel_initializer=glorot_uniform(seed=0),\n",
        "               kernel_regularizer=tf.keras.regularizers.l2(l2_reg))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
        "\n",
        "    # Shortcut path\n",
        "    X_shortcut = Conv2D(F3, (1, 1), strides=(s, s), name=conv_name_base + '1', padding='valid',\n",
        "                        kernel_initializer=glorot_uniform(seed=0),\n",
        "                        kernel_regularizer=tf.keras.regularizers.l2(l2_reg))(X_shortcut)\n",
        "    X_shortcut = BatchNormalization(axis=3, name=bn_name_base + '1')(X_shortcut)\n",
        "\n",
        "    # Add shortcut value to main path\n",
        "    X = Add()([X_shortcut, X])\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    return X\n",
        "\n",
        "def ResNet50(input_shape=(256, 256, 3), classes=1):\n",
        "    X_input = Input(input_shape)\n",
        "\n",
        "    X = ZeroPadding2D((3, 3))(X_input)\n",
        "\n",
        "    # Stage 1\n",
        "    X = Conv2D(64, (7, 7), strides=(2, 2), name='conv1', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name='bn_conv1')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
        "\n",
        "    # Stage 2\n",
        "    X = convolutional_block(X, f=3, filters=[64, 64, 256], stage=2, block='a', s=1)\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n",
        "\n",
        "    # Stage 3\n",
        "    X = convolutional_block(X, f=3, filters=[128, 128, 512], stage=3, block='a', s=2)\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='b')\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='c')\n",
        "\n",
        "    # Stage 4\n",
        "    X = convolutional_block(X, f=3, filters=[256, 256, 1024], stage=4, block='a', s=2)\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='b')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='c')\n",
        "\n",
        "    # Average Pooling\n",
        "    X = AveragePooling2D(pool_size=(2, 2))(X)\n",
        "\n",
        "    # Instead of Flatten, directly add the output layer\n",
        "    X = Conv2D(classes, (1, 1), activation='sigmoid', name='output')(X)\n",
        "\n",
        "    # Upsample to the desired output size (adjust this as needed)\n",
        "    X = UpSampling2D(size=(32, 32), interpolation='bilinear')(X)\n",
        "\n",
        "    # Update the model creation\n",
        "    model = tf.keras.models.Model(inputs=X_input, outputs=X)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Compile the model\n",
        "model = ResNet50(input_shape=(256, 256, 3), classes=1)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Use your data loading function and train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ky-AM7SUkYTl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2  # or another image processing library\n",
        "import numpy as np\n",
        "\n",
        "def load_full_dataset(image_folder, mask_folder):\n",
        "    images = []\n",
        "    masks = []\n",
        "\n",
        "    # Get all image and mask file paths\n",
        "    image_filenames = sorted(os.listdir(image_folder))\n",
        "    mask_filenames = sorted(os.listdir(mask_folder))\n",
        "\n",
        "    for img_file, mask_file in zip(image_filenames, mask_filenames):\n",
        "        # Load images\n",
        "        img_path = os.path.join(image_folder, img_file)\n",
        "        mask_path = os.path.join(mask_folder, mask_file)\n",
        "\n",
        "        # Read and preprocess the image and mask (resize, normalize, etc.)\n",
        "        image = cv2.imread(img_path)  # Load image\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # Load mask in grayscale\n",
        "\n",
        "        if image is not None and mask is not None:\n",
        "            # Resize images to the required input shape (256, 256)\n",
        "            image = cv2.resize(image, (256, 256))\n",
        "            mask = cv2.resize(mask, (256, 256))\n",
        "\n",
        "            # Normalize image data to [0, 1] range if needed\n",
        "            image = image.astype('float32') / 255.0\n",
        "            mask = mask.astype('float32') / 255.0  # For binary masks, you can also keep it 0-1\n",
        "\n",
        "            # Append to lists\n",
        "            images.append(image)\n",
        "            masks.append(mask)\n",
        "\n",
        "    return np.array(images), np.array(masks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LILQJjdkJAu",
        "outputId": "f68f32c2-1dca-4e40-e29a-b8132e8b727d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "3/3 [==============================] - ETA: 0s - loss: 65.9503 - accuracy: 0.7362"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r3/3 [==============================] - 16s 2s/step - loss: 65.9503 - accuracy: 0.7362 - val_loss: 62.2402 - val_accuracy: 0.9243\n",
            "Epoch 2/10\n",
            "3/3 [==============================] - 6s 2s/step - loss: 60.6560 - accuracy: 0.9286 - val_loss: 56.8020 - val_accuracy: 0.9213\n",
            "Epoch 3/10\n",
            "3/3 [==============================] - 6s 2s/step - loss: 55.1733 - accuracy: 0.9211 - val_loss: 51.2371 - val_accuracy: 0.9243\n",
            "Epoch 4/10\n",
            "3/3 [==============================] - 6s 2s/step - loss: 49.7287 - accuracy: 0.9252 - val_loss: 45.9297 - val_accuracy: 0.9243\n",
            "Epoch 5/10\n",
            "3/3 [==============================] - 6s 2s/step - loss: 44.5515 - accuracy: 0.9270 - val_loss: 40.9731 - val_accuracy: 0.9243\n",
            "Epoch 6/10\n",
            "3/3 [==============================] - 5s 2s/step - loss: 39.7001 - accuracy: 0.9302 - val_loss: 36.4092 - val_accuracy: 0.9243\n",
            "Epoch 7/10\n",
            "3/3 [==============================] - 5s 2s/step - loss: 35.2539 - accuracy: 0.9261 - val_loss: 32.2551 - val_accuracy: 0.9243\n",
            "Epoch 8/10\n",
            "3/3 [==============================] - 5s 2s/step - loss: 31.2047 - accuracy: 0.9292 - val_loss: 28.5187 - val_accuracy: 0.9243\n",
            "Epoch 9/10\n",
            "3/3 [==============================] - 5s 2s/step - loss: 27.5855 - accuracy: 0.9275 - val_loss: 25.1812 - val_accuracy: 0.9243\n",
            "Epoch 10/10\n",
            "3/3 [==============================] - 5s 2s/step - loss: 24.3353 - accuracy: 0.9291 - val_loss: 22.2217 - val_accuracy: 0.9243\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Compile the model\n",
        "model = ResNet50(input_shape=(256, 256, 3), classes=1)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Load your entire dataset\n",
        "all_images, all_masks = load_full_dataset(train_image_folder, train_mask_folder)\n",
        "\n",
        "# Ensure the data is in the right shape\n",
        "all_images = np.array(all_images)  # Shape: (num_samples, 256, 256, 3)\n",
        "all_masks = np.array(all_masks)     # Shape: (num_samples, 256, 256, 1)\n",
        "\n",
        "# Define your batch size\n",
        "batch_size = 32  # Adjust this based on your system's memory\n",
        "\n",
        "# Define the number of epochs\n",
        "epochs = 10  # You can adjust this\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True)\n",
        "\n",
        "# Train the model\n",
        "model.fit(all_images, all_masks,\n",
        "          epochs=epochs,\n",
        "          batch_size=batch_size,\n",
        "          validation_split=0.2,  # Use a validation split for validation data\n",
        "          callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "# Optionally, save the final model after training\n",
        "model.save('final_resnet50_model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZkUCxWfkJDH"
      },
      "outputs": [],
      "source": [
        "#LOSS IS HIGH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROSAiUUWp0bo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.layers import Input, Dense, Conv2D, BatchNormalization, Activation, Add, AveragePooling2D, ZeroPadding2D, MaxPooling2D, Dropout\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.initializers import glorot_uniform\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJZKXYn0kJFl"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Conv2D, BatchNormalization, Activation, Add,\n",
        "    ZeroPadding2D, MaxPooling2D, AveragePooling2D, Dropout,\n",
        "    UpSampling2D\n",
        ")\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "\n",
        "# Define the ResNet50 architecture\n",
        "def identity_block(X, f, filters, stage, block, l2_reg=0.01):\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "    F1, F2, F3 = filters\n",
        "\n",
        "    X_shortcut = X\n",
        "\n",
        "    # First component of main path\n",
        "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(1, 1), padding='valid',\n",
        "               name=conv_name_base + '2a', kernel_initializer=glorot_uniform(seed=0),\n",
        "               kernel_regularizer=tf.keras.regularizers.l2(l2_reg))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Second component of main path\n",
        "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same',\n",
        "               name=conv_name_base + '2b', kernel_initializer=glorot_uniform(seed=0),\n",
        "               kernel_regularizer=tf.keras.regularizers.l2(l2_reg))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path\n",
        "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid',\n",
        "               name=conv_name_base + '2c', kernel_initializer=glorot_uniform(seed=0),\n",
        "               kernel_regularizer=tf.keras.regularizers.l2(l2_reg))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
        "\n",
        "    # Add shortcut value to main path\n",
        "    X = Add()([X_shortcut, X])\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    return X\n",
        "\n",
        "def convolutional_block(X, f, filters, stage, block, s=2, l2_reg=0.01):\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "    F1, F2, F3 = filters\n",
        "\n",
        "    X_shortcut = X\n",
        "\n",
        "    # First component of main path\n",
        "    X = Conv2D(F1, (1, 1), strides=(s, s), name=conv_name_base + '2a', padding='valid',\n",
        "               kernel_initializer=glorot_uniform(seed=0),\n",
        "               kernel_regularizer=tf.keras.regularizers.l2(l2_reg))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Second component of main path\n",
        "    X = Conv2D(F2, (f, f), strides=(1, 1), name=conv_name_base + '2b', padding='same',\n",
        "               kernel_initializer=glorot_uniform(seed=0),\n",
        "               kernel_regularizer=tf.keras.regularizers.l2(l2_reg))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path\n",
        "    X = Conv2D(F3, (1, 1), strides=(1, 1), name=conv_name_base + '2c', padding='valid',\n",
        "               kernel_initializer=glorot_uniform(seed=0),\n",
        "               kernel_regularizer=tf.keras.regularizers.l2(l2_reg))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
        "\n",
        "    # Shortcut path\n",
        "    X_shortcut = Conv2D(F3, (1, 1), strides=(s, s), name=conv_name_base + '1', padding='valid',\n",
        "                        kernel_initializer=glorot_uniform(seed=0),\n",
        "                        kernel_regularizer=tf.keras.regularizers.l2(l2_reg))(X_shortcut)\n",
        "    X_shortcut = BatchNormalization(axis=3, name=bn_name_base + '1')(X_shortcut)\n",
        "\n",
        "    # Add shortcut value to main path\n",
        "    X = Add()([X_shortcut, X])\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    return X\n",
        "\n",
        "def ResNet50(input_shape=(256, 256, 3), classes=1):\n",
        "    X_input = Input(input_shape)\n",
        "\n",
        "    X = ZeroPadding2D((3, 3))(X_input)\n",
        "\n",
        "    # Stage 1\n",
        "    X = Conv2D(64, (7, 7), strides=(2, 2), name='conv1', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name='bn_conv1')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
        "\n",
        "    # Stage 2\n",
        "    X = convolutional_block(X, f=3, filters=[64, 64, 256], stage=2, block='a', s=1)\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n",
        "\n",
        "    # Stage 3\n",
        "    X = convolutional_block(X, f=3, filters=[128, 128, 512], stage=3, block='a', s=2)\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='b')\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='c')\n",
        "\n",
        "    # Stage 4\n",
        "    X = convolutional_block(X, f=3, filters=[256, 256, 1024], stage=4, block='a', s=2)\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='b')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='c')\n",
        "\n",
        "    # Average Pooling\n",
        "    X = AveragePooling2D(pool_size=(2, 2))(X)\n",
        "\n",
        "    # Add Dropout layer\n",
        "    X = Dropout(0.5)(X)\n",
        "\n",
        "    # Instead of Flatten, directly add the output layer\n",
        "    X = Conv2D(classes, (1, 1), activation='sigmoid', name='output')(X)\n",
        "\n",
        "    # Upsample to the desired output size (adjust this as needed)\n",
        "    X = UpSampling2D(size=(32, 32), interpolation='bilinear')(X)\n",
        "\n",
        "    # Update the model creation with a name\n",
        "    model = tf.keras.models.Model(inputs=X_input, outputs=X, name='ResNet50')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Compile the model\n",
        "model = ResNet50(input_shape=(256, 256, 3), classes=1)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Use your data loading function and train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtWuxpHbkJH8",
        "outputId": "e78ea717-1bc8-4b9e-8e6b-b74e362400ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded images shape: (99, 256, 256, 3)\n",
            "Loaded masks shape: (99, 256, 256, 1)\n",
            "Normalized images shape: (99, 256, 256, 3)\n",
            "Normalized masks shape: (99, 256, 256, 1)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def load_full_dataset(train_image_folder, train_mask_folder):\n",
        "    images = []\n",
        "    masks = []\n",
        "\n",
        "    # List all image files in the image folder\n",
        "    image_files = sorted(os.listdir(train_image_folder))\n",
        "    mask_files = sorted(os.listdir(train_mask_folder))\n",
        "\n",
        "    for img_file, mask_file in zip(image_files, mask_files):\n",
        "        # Construct full file path\n",
        "        img_path = os.path.join(train_image_folder, img_file)\n",
        "        mask_path = os.path.join(train_mask_folder, mask_file)\n",
        "\n",
        "        # Load the image\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            print(f\"Warning: Unable to read image {img_path}\")\n",
        "            continue\n",
        "        img = cv2.resize(img, (256, 256))  # Resize to match the model input\n",
        "\n",
        "        # Load the mask\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # Read as grayscale\n",
        "        if mask is None:\n",
        "            print(f\"Warning: Unable to read mask {mask_path}\")\n",
        "            continue\n",
        "        mask = cv2.resize(mask, (256, 256))  # Resize to match the model input\n",
        "\n",
        "        # Append to lists\n",
        "        images.append(img)\n",
        "        masks.append(mask.reshape((256, 256, 1)))  # Add channel dimension to masks\n",
        "\n",
        "    return np.array(images), np.array(masks)\n",
        "\n",
        "# Specify your folder paths\n",
        "train_image_folder = '/content/drive/MyDrive/iirs/small_data/processed_raster_mask_tiles'\n",
        "train_mask_folder = '/content/drive/MyDrive/iirs/small_data/processed_raster_mask_tiles'\n",
        "\n",
        "# Load your entire dataset\n",
        "all_images, all_masks = load_full_dataset(train_image_folder, train_mask_folder)\n",
        "\n",
        "# Ensure the data is in the right shape\n",
        "print(f\"Loaded images shape: {all_images.shape}\")  # Should be (99, 256, 256, 3)\n",
        "print(f\"Loaded masks shape: {all_masks.shape}\")    # Should be (99, 256, 256, 1)\n",
        "\n",
        "# Normalize images if necessary\n",
        "all_images = all_images.astype('float32') / 255.0\n",
        "all_masks = all_masks.astype('float32') / 255.0  # Ensure masks are also in the right format\n",
        "\n",
        "# Check the normalized data shapes\n",
        "print(f\"Normalized images shape: {all_images.shape}\")\n",
        "print(f\"Normalized masks shape: {all_masks.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGwUumCJqjj4"
      },
      "outputs": [],
      "source": [
        "model = ResNet50(input_shape=(256, 256, 3), classes=1)  # Update the input shape and class count as needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDY1AS7LqjnY"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evBKzaRGqtYY"
      },
      "outputs": [],
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=1e-6)\n",
        "model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAg_kfg4qtfK",
        "outputId": "7b3846c4-a9fe-4d9e-8488-989f6b671a7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 66.5246 - accuracy: 0.4257"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r3/3 [==============================] - 16s 2s/step - loss: 66.5246 - accuracy: 0.4257 - val_loss: 63.1843 - val_accuracy: 0.5546 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 61.0306 - accuracy: 0.9294 - val_loss: 58.4365 - val_accuracy: 0.7787 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 56.5772 - accuracy: 0.9252 - val_loss: 53.3285 - val_accuracy: 0.9317 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 51.6521 - accuracy: 0.9340 - val_loss: 48.4529 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 47.1814 - accuracy: 0.9318 - val_loss: 43.7973 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 42.5955 - accuracy: 0.9306 - val_loss: 39.4080 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 37.9908 - accuracy: 0.9339 - val_loss: 35.3227 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 34.0360 - accuracy: 0.9294 - val_loss: 31.5799 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 30.4125 - accuracy: 0.9344 - val_loss: 28.1715 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 27.3321 - accuracy: 0.9313 - val_loss: 25.0910 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 24.3385 - accuracy: 0.9342 - val_loss: 22.3250 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 12/50\n",
            "3/3 [==============================] - 5s 2s/step - loss: 21.2980 - accuracy: 0.9295 - val_loss: 19.8558 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 13/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 19.2647 - accuracy: 0.9344 - val_loss: 17.6580 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 14/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 16.9943 - accuracy: 0.9345 - val_loss: 15.7038 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 15/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 15.2388 - accuracy: 0.9336 - val_loss: 13.9712 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 16/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 13.4555 - accuracy: 0.9334 - val_loss: 12.4382 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 17/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 11.9918 - accuracy: 0.9276 - val_loss: 11.0809 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 18/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 10.5829 - accuracy: 0.9317 - val_loss: 9.8811 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 19/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 9.4388 - accuracy: 0.9300 - val_loss: 8.8186 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 20/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 8.4301 - accuracy: 0.9327 - val_loss: 7.8786 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 21/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 7.5923 - accuracy: 0.9342 - val_loss: 7.0465 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 22/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 6.7393 - accuracy: 0.9300 - val_loss: 6.3121 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 23/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 6.1438 - accuracy: 0.9287 - val_loss: 5.6674 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 24/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 5.4273 - accuracy: 0.9304 - val_loss: 5.0979 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 25/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 4.9626 - accuracy: 0.9322 - val_loss: 4.5927 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 26/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 4.4915 - accuracy: 0.9279 - val_loss: 4.1451 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 27/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 4.0359 - accuracy: 0.9308 - val_loss: 3.7472 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 28/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 3.6420 - accuracy: 0.9341 - val_loss: 3.3926 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 29/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 3.3039 - accuracy: 0.9249 - val_loss: 3.0856 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 30/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 2.9969 - accuracy: 0.9316 - val_loss: 2.8409 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 31/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 2.7618 - accuracy: 0.9300 - val_loss: 2.6295 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 32/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 2.5692 - accuracy: 0.9229 - val_loss: 2.4369 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 33/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 2.3859 - accuracy: 0.9206 - val_loss: 2.2600 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 34/50\n",
            "3/3 [==============================] - 5s 2s/step - loss: 2.1762 - accuracy: 0.9331 - val_loss: 2.0919 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 35/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 2.0165 - accuracy: 0.9318 - val_loss: 1.9346 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 36/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 1.8965 - accuracy: 0.9325 - val_loss: 1.7861 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 37/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 1.7412 - accuracy: 0.9328 - val_loss: 1.6507 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 38/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 1.6064 - accuracy: 0.9328 - val_loss: 1.5269 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 39/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 1.4879 - accuracy: 0.9336 - val_loss: 1.4133 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 40/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 1.3692 - accuracy: 0.9342 - val_loss: 1.3144 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 41/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 1.2905 - accuracy: 0.9267 - val_loss: 1.2274 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 42/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 1.2067 - accuracy: 0.9311 - val_loss: 1.1525 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 43/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 1.1312 - accuracy: 0.9322 - val_loss: 1.0859 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 44/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 1.0660 - accuracy: 0.9337 - val_loss: 1.0243 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 45/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 0.9883 - accuracy: 0.9343 - val_loss: 0.9658 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 46/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 0.9373 - accuracy: 0.9345 - val_loss: 0.9117 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 47/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 0.8838 - accuracy: 0.9343 - val_loss: 0.8620 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 48/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 0.8421 - accuracy: 0.9303 - val_loss: 0.8173 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 49/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 0.7834 - accuracy: 0.9338 - val_loss: 0.7756 - val_accuracy: 0.9323 - lr: 0.0010\n",
            "Epoch 50/50\n",
            "3/3 [==============================] - 6s 2s/step - loss: 0.7562 - accuracy: 0.9330 - val_loss: 0.7387 - val_accuracy: 0.9323 - lr: 0.0010\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f3525cca4d0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_images, val_images, train_masks, val_masks = train_test_split(\n",
        "    all_images, all_masks, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define the data generator\n",
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "epochs=50\n",
        "# Define your batch size\n",
        "batch_size = 32  # Adjust this based on your system's memory\n",
        "\n",
        "# Fit the model\n",
        "model.fit(datagen.flow(train_images, train_masks, batch_size=batch_size),\n",
        "          validation_data=(val_images, val_masks),  # Use the separate validation set\n",
        "          epochs=epochs,\n",
        "          callbacks=[early_stopping, reduce_lr, model_checkpoint])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yJVntd9rgVf"
      },
      "outputs": [],
      "source": [
        "# Save the model to a specific path\n",
        "model.save('/content/drive/MyDrive/saved_model/my_resnet50_model_final')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LSL_44LKb1z_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2QjeAIayisz"
      },
      "outputs": [],
      "source": [
        "model.save_weights('/content/drive/MyDrive/saved_model/my_model_weights.h5')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RESNET+UNET\n",
        "**bold text**"
      ],
      "metadata": {
        "id": "J6-M9dZWH21O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndAie5rjgaH4",
        "outputId": "9b2c2fd2-41d2-48ae-d9e8-69a1bc9b2ffd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_5 (InputLayer)        [(None, 256, 256, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " zero_padding2d_4 (ZeroPadd  (None, 262, 262, 3)          0         ['input_5[0][0]']             \n",
            " ing2D)                                                                                           \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)             (None, 128, 128, 64)         9472      ['zero_padding2d_4[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization (Batch  (None, 128, 128, 64)         256       ['conv2d[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_112 (Activation  (None, 128, 128, 64)         0         ['batch_normalization[0][0]'] \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " max_pooling2d_4 (MaxPoolin  (None, 64, 64, 64)           0         ['activation_112[0][0]']      \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)           (None, 64, 64, 64)           4160      ['max_pooling2d_4[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_1 (Bat  (None, 64, 64, 64)           256       ['conv2d_1[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_113 (Activation  (None, 64, 64, 64)           0         ['batch_normalization_1[0][0]'\n",
            " )                                                                  ]                             \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)           (None, 64, 64, 64)           36928     ['activation_113[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_2 (Bat  (None, 64, 64, 64)           256       ['conv2d_2[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_114 (Activation  (None, 64, 64, 64)           0         ['batch_normalization_2[0][0]'\n",
            " )                                                                  ]                             \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)           (None, 64, 64, 256)          16640     ['activation_114[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)           (None, 64, 64, 256)          16640     ['max_pooling2d_4[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_3 (Bat  (None, 64, 64, 256)          1024      ['conv2d_3[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " add_36 (Add)                (None, 64, 64, 256)          0         ['batch_normalization_3[0][0]'\n",
            "                                                                    , 'batch_normalization_4[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_115 (Activation  (None, 64, 64, 256)          0         ['add_36[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)           (None, 32, 32, 128)          32896     ['activation_115[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_5 (Bat  (None, 32, 32, 128)          512       ['conv2d_5[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_116 (Activation  (None, 32, 32, 128)          0         ['batch_normalization_5[0][0]'\n",
            " )                                                                  ]                             \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)           (None, 32, 32, 128)          147584    ['activation_116[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_6 (Bat  (None, 32, 32, 128)          512       ['conv2d_6[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_117 (Activation  (None, 32, 32, 128)          0         ['batch_normalization_6[0][0]'\n",
            " )                                                                  ]                             \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)           (None, 32, 32, 512)          66048     ['activation_117[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)           (None, 32, 32, 512)          131584    ['activation_115[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_8 (Bat  (None, 32, 32, 512)          2048      ['conv2d_8[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " add_37 (Add)                (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'\n",
            "                                                                    , 'batch_normalization_8[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_118 (Activation  (None, 32, 32, 512)          0         ['add_37[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)           (None, 16, 16, 256)          131328    ['activation_118[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_9 (Bat  (None, 16, 16, 256)          1024      ['conv2d_9[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_119 (Activation  (None, 16, 16, 256)          0         ['batch_normalization_9[0][0]'\n",
            " )                                                                  ]                             \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)          (None, 16, 16, 256)          590080    ['activation_119[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_10 (Ba  (None, 16, 16, 256)          1024      ['conv2d_10[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_120 (Activation  (None, 16, 16, 256)          0         ['batch_normalization_10[0][0]\n",
            " )                                                                  ']                            \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)          (None, 16, 16, 1024)         263168    ['activation_120[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)          (None, 16, 16, 1024)         525312    ['activation_118[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_11 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_11[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_12 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_12[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_38 (Add)                (None, 16, 16, 1024)         0         ['batch_normalization_11[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'batch_normalization_12[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_121 (Activation  (None, 16, 16, 1024)         0         ['add_38[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)          (None, 8, 8, 512)            524800    ['activation_121[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_13 (Ba  (None, 8, 8, 512)            2048      ['conv2d_13[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_122 (Activation  (None, 8, 8, 512)            0         ['batch_normalization_13[0][0]\n",
            " )                                                                  ']                            \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)          (None, 8, 8, 512)            2359808   ['activation_122[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_14 (Ba  (None, 8, 8, 512)            2048      ['conv2d_14[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_123 (Activation  (None, 8, 8, 512)            0         ['batch_normalization_14[0][0]\n",
            " )                                                                  ']                            \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)          (None, 8, 8, 2048)           1050624   ['activation_123[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)          (None, 8, 8, 2048)           2099200   ['activation_121[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_15 (Ba  (None, 8, 8, 2048)           8192      ['conv2d_15[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_16 (Ba  (None, 8, 8, 2048)           8192      ['conv2d_16[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_39 (Add)                (None, 8, 8, 2048)           0         ['batch_normalization_15[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'batch_normalization_16[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_124 (Activation  (None, 8, 8, 2048)           0         ['add_39[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " up_sampling2d_4 (UpSamplin  (None, 16, 16, 2048)         0         ['activation_124[0][0]']      \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 16, 16, 3072)         0         ['up_sampling2d_4[0][0]',     \n",
            "                                                                     'activation_121[0][0]']      \n",
            "                                                                                                  \n",
            " up_sampling2d_5 (UpSamplin  (None, 32, 32, 3072)         0         ['concatenate[0][0]']         \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate  (None, 32, 32, 3584)         0         ['up_sampling2d_5[0][0]',     \n",
            " )                                                                   'activation_118[0][0]']      \n",
            "                                                                                                  \n",
            " up_sampling2d_6 (UpSamplin  (None, 64, 64, 3584)         0         ['concatenate_1[0][0]']       \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate  (None, 64, 64, 3840)         0         ['up_sampling2d_6[0][0]',     \n",
            " )                                                                   'activation_115[0][0]']      \n",
            "                                                                                                  \n",
            " up_sampling2d_7 (UpSamplin  (None, 128, 128, 3840)       0         ['concatenate_2[0][0]']       \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate  (None, 128, 128, 3904)       0         ['up_sampling2d_7[0][0]',     \n",
            " )                                                                   'activation_112[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)          (None, 128, 128, 1)          3905      ['concatenate_3[0][0]']       \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8048833 (30.70 MB)\n",
            "Trainable params: 8029505 (30.63 MB)\n",
            "Non-trainable params: 19328 (75.50 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "def ResNet_UNet(input_shape=(256, 256, 3)):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Stage 1: Initial Conv + ZeroPadding\n",
        "    x = layers.ZeroPadding2D(padding=(3, 3))(inputs)\n",
        "    x = layers.Conv2D(64, (7, 7), strides=(2, 2), padding='valid')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    skip1 = x  # Skip connection 1 (Shape: 128x128)\n",
        "\n",
        "    # Stage 2: MaxPool + ResNet Block 1 (conv2_x)\n",
        "    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "    x = res_block(x, [64, 64, 256], strides=1)  # conv2_x block\n",
        "    skip2 = x  # Skip connection 2 (Shape: 64x64)\n",
        "\n",
        "    # Stage 3: ResNet Block 2 (conv3_x)\n",
        "    x = res_block(x, [128, 128, 512], strides=2)  # conv3_x block\n",
        "    skip3 = x  # Skip connection 3 (Shape: 32x32)\n",
        "\n",
        "    # Stage 4: ResNet Block 3 (conv4_x)\n",
        "    x = res_block(x, [256, 256, 1024], strides=2)  # conv4_x block\n",
        "    skip4 = x  # Skip connection 4 (Shape: 16x16)\n",
        "\n",
        "    # Stage 5: ResNet Block 4 (conv5_x)\n",
        "    x = res_block(x, [512, 512, 2048], strides=2)  # conv5_x block\n",
        "\n",
        "    # Decoder Stage 1: Up-sample and concatenate with skip connection 4 (16x16)\n",
        "    x = layers.UpSampling2D((2, 2))(x)\n",
        "    x = layers.Concatenate()([x, skip4])  # 16x16\n",
        "\n",
        "    # Decoder Stage 2: Up-sample and concatenate with skip connection 3 (32x32)\n",
        "    x = layers.UpSampling2D((2, 2))(x)\n",
        "    x = layers.Concatenate()([x, skip3])  # 32x32\n",
        "\n",
        "    # Decoder Stage 3: Up-sample and concatenate with skip connection 2 (64x64)\n",
        "    x = layers.UpSampling2D((2, 2))(x)\n",
        "    x = layers.Concatenate()([x, skip2])  # 64x64\n",
        "\n",
        "    # Decoder Stage 4: Up-sample and concatenate with skip connection 1 (128x128)\n",
        "    x = layers.UpSampling2D((2, 2))(x)\n",
        "    x = layers.Concatenate()([x, skip1])  # 128x128\n",
        "\n",
        "    # Final Convolution to return to original resolution\n",
        "    x = layers.Conv2D(1, (1, 1), activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs, x)\n",
        "    return model\n",
        "\n",
        "def res_block(x, filters, strides=1):\n",
        "    # Filters = [filter1, filter2, filter3]\n",
        "    filter1, filter2, filter3 = filters\n",
        "\n",
        "    # First branch\n",
        "    shortcut = x\n",
        "    x = layers.Conv2D(filter1, (1, 1), strides=strides)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    x = layers.Conv2D(filter2, (3, 3), padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    x = layers.Conv2D(filter3, (1, 1))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # Shortcut connection\n",
        "    shortcut = layers.Conv2D(filter3, (1, 1), strides=strides)(shortcut)\n",
        "    shortcut = layers.BatchNormalization()(shortcut)\n",
        "\n",
        "    # Adding shortcut and main path\n",
        "    x = layers.Add()([x, shortcut])\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "# Compile and build the model\n",
        "model = ResNet_UNet()\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwBtPw9M2Ug9"
      },
      "outputs": [],
      "source": [
        "# Save the model in HDF5 format\n",
        "model.save('my_resnet50_model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fzy_20to2U4r",
        "outputId": "799f1c36-2286-47df-f276-6c907a6d8fdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tf2onnx\n",
            "  Downloading tf2onnx-1.16.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (1.16.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (24.3.25)\n",
            "Requirement already satisfied: protobuf~=3.20 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (3.20.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (2024.8.30)\n",
            "Downloading tf2onnx-1.16.1-py3-none-any.whl (455 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.8/455.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx, tf2onnx\n",
            "Successfully installed onnx-1.17.0 tf2onnx-1.16.1\n"
          ]
        }
      ],
      "source": [
        "pip install tf2onnx onnx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72_dgtcc2bBQ"
      },
      "outputs": [],
      "source": [
        "import tf2onnx\n",
        "import tensorflow as tf\n",
        "import onnx  # Import the onnx module\n",
        "\n",
        "# Load your TensorFlow model\n",
        "model = tf.keras.models.load_model('my_resnet50_model.h5')\n",
        "\n",
        "# Convert the TensorFlow model to ONNX format\n",
        "onnx_model, _ = tf2onnx.convert.from_keras(model)\n",
        "\n",
        "# Save the ONNX model\n",
        "onnx.save_model(onnx_model, 'my_resnet50_model.onnx')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sHKB_lv2xkE",
        "outputId": "9712e8b7-095c-488e-8772-8a2916cc750e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (1.17.0)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.3.25)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.13.3)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m104.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.19.2\n"
          ]
        }
      ],
      "source": [
        "pip install onnxruntime onnx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZgg5mZh24ag",
        "outputId": "e166bc1f-c51c-4b4b-ba0a-90ad2386ea3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx2pytorch\n",
            "  Downloading onnx2pytorch-0.5.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from onnx2pytorch) (2.4.0+cpu)\n",
            "Requirement already satisfied: onnx>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from onnx2pytorch) (1.17.0)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from onnx2pytorch) (0.19.0+cpu)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.6.0->onnx2pytorch) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.6.0->onnx2pytorch) (3.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->onnx2pytorch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->onnx2pytorch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->onnx2pytorch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->onnx2pytorch) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->onnx2pytorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->onnx2pytorch) (2024.9.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.9.0->onnx2pytorch) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->onnx2pytorch) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->onnx2pytorch) (1.3.0)\n",
            "Downloading onnx2pytorch-0.5.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx2pytorch\n",
            "Successfully installed onnx2pytorch-0.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install onnx2pytorch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBqrUmF43G_A"
      },
      "outputs": [],
      "source": [
        "import onnx2pytorch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iH6_t_zB3J7n",
        "outputId": "8d220953-f949-4336-dee5-6c100b2ee40f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
            "  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n",
            "/usr/local/lib/python3.10/dist-packages/onnx2pytorch/convert/attribute.py:101: UserWarning: Pytorch's interpolate uses no coordinate_transformation_mode=tf_half_pixel_for_nn. Result might differ.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/onnx2pytorch/operations/resize.py:16: UserWarning: Pytorch's interpolate uses no exclude_outside. Result might differ.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/onnx2pytorch/operations/resize.py:16: UserWarning: Pytorch's interpolate uses no nearest_mode. Result might differ.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import onnx\n",
        "import onnx2pytorch\n",
        "\n",
        "# Load the ONNX model\n",
        "onnx_model = onnx.load('my_resnet50_model.onnx')\n",
        "\n",
        "# Convert ONNX model to PyTorch model\n",
        "pytorch_model = onnx2pytorch.ConvertModel(onnx_model)\n",
        "\n",
        "# Now you can use pytorch_model as a regular PyTorch model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyRkQeWiiZOh",
        "outputId": "51696d08-44dc-4938-b445-9bde11da721d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train images shape: (1243, 256, 256, 3)\n",
            "Train masks shape: (1243, 256, 256, 1)\n",
            "Validation images shape: (311, 256, 256, 3)\n",
            "Validation masks shape: (311, 256, 256, 1)\n"
          ]
        }
      ],
      "source": [
        "print(f'Train images shape: {train_images.shape}')\n",
        "print(f'Train masks shape: {train_masks.shape}')\n",
        "print(f'Validation images shape: {val_images.shape}')\n",
        "print(f'Validation masks shape: {val_masks.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQb1WbSuq20m"
      },
      "source": [
        "STEPS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjrs0fvTqSr_",
        "outputId": "cc8eaff4-15ec-467b-8666-ba4eccb286cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (1.17.0)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.10/dist-packages (1.19.2)\n",
            "Requirement already satisfied: onnx2pytorch in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.3.25)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.13.3)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from onnx2pytorch) (2.4.0+cpu)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from onnx2pytorch) (0.19.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->onnx2pytorch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->onnx2pytorch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->onnx2pytorch) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->onnx2pytorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->onnx2pytorch) (2024.9.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.9.0->onnx2pytorch) (11.0.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->onnx2pytorch) (3.0.1)\n"
          ]
        }
      ],
      "source": [
        "pip install onnx onnxruntime onnx2pytorch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lrn63Y9D4eLf",
        "outputId": "9f184d46-af2c-4e4b-e6d6-76a14a88de70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 209MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "# Define the backbone as 'resnet50'\n",
        "backbone = 'resnet50'\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    U-Net's decoding module.\n",
        "    Uses a 3x3 convolution followed by an upsampling layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, mid_channels, out_channels, upsample_mode='pixelshuffle', BN_enable=True):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.mid_channels = mid_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.upsample_mode = upsample_mode\n",
        "        self.BN_enable = BN_enable\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=mid_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        if self.BN_enable:\n",
        "            self.norm1 = nn.BatchNorm2d(mid_channels)\n",
        "        self.relu1 = nn.ReLU(inplace=False)\n",
        "        self.relu2 = nn.ReLU(inplace=False)\n",
        "\n",
        "        if self.upsample_mode == 'deconv':\n",
        "            self.upsample = nn.ConvTranspose2d(in_channels=mid_channels, out_channels=out_channels,\n",
        "                                                kernel_size=3, stride=2, padding=1, output_padding=1, bias=False)\n",
        "        elif self.upsample_mode == 'pixelshuffle':\n",
        "            self.upsample = nn.PixelShuffle(upscale_factor=2)\n",
        "        if self.BN_enable:\n",
        "            self.norm2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        if self.BN_enable:\n",
        "            x = self.norm1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.upsample(x)\n",
        "        if self.BN_enable:\n",
        "            x = self.norm2(x)\n",
        "        x = self.relu2(x)\n",
        "        return x\n",
        "\n",
        "class Resnet_Unet(nn.Module):\n",
        "    \"\"\"\n",
        "    U-Net model using ResNet-50 as the backbone.\n",
        "    \"\"\"\n",
        "    def __init__(self, BN_enable=True):\n",
        "        super().__init__()\n",
        "        self.BN_enable = BN_enable\n",
        "\n",
        "        # encoder part\n",
        "        # Using the pretrained ResNet-50 model directly from torchvision\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "\n",
        "        filters = [64, 256, 512, 1024, 2048]\n",
        "        self.firstconv = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.firstbn = resnet.bn1\n",
        "        self.firstrelu = resnet.relu\n",
        "        self.firstmaxpool = resnet.maxpool\n",
        "        self.encoder1 = resnet.layer1\n",
        "        self.encoder2 = resnet.layer2\n",
        "        self.encoder3 = resnet.layer3\n",
        "\n",
        "        # decoder part\n",
        "        self.center = DecoderBlock(in_channels=filters[3], mid_channels=filters[3]*4, out_channels=filters[3], BN_enable=self.BN_enable)\n",
        "        self.decoder1 = DecoderBlock(in_channels=filters[3]+filters[2], mid_channels=filters[2]*4, out_channels=filters[2], BN_enable=self.BN_enable)\n",
        "        self.decoder2 = DecoderBlock(in_channels=filters[2]+filters[1], mid_channels=filters[1]*4, out_channels=filters[1], BN_enable=self.BN_enable)\n",
        "        self.decoder3 = DecoderBlock(in_channels=filters[1]+filters[0], mid_channels=filters[0]*4, out_channels=filters[0], BN_enable=self.BN_enable)\n",
        "\n",
        "        if self.BN_enable:\n",
        "            self.final = nn.Sequential(\n",
        "                nn.Conv2d(in_channels=filters[0], out_channels=32, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(32),\n",
        "                nn.ReLU(inplace=False),\n",
        "                nn.Conv2d(in_channels=32, out_channels=1, kernel_size=1),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "        else:\n",
        "            self.final = nn.Sequential(\n",
        "                nn.Conv2d(in_channels=filters[0], out_channels=32, kernel_size=3, padding=1),\n",
        "                nn.ReLU(inplace=False),\n",
        "                nn.Conv2d(in_channels=32, out_channels=1, kernel_size=1),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.firstconv(x)\n",
        "        x = self.firstbn(x)\n",
        "        x = self.firstrelu(x)\n",
        "        x_ = self.firstmaxpool(x)\n",
        "\n",
        "        e1 = self.encoder1(x_)\n",
        "        e2 = self.encoder2(e1)\n",
        "        e3 = self.encoder3(e2)\n",
        "\n",
        "        center = self.center(e3)\n",
        "\n",
        "        d2 = self.decoder1(torch.cat([center, e2], dim=1))\n",
        "        d3 = self.decoder2(torch.cat([d2, e1], dim=1))\n",
        "        d4 = self.decoder3(torch.cat([d3, x], dim=1))\n",
        "\n",
        "        return self.final(d4)\n",
        "\n",
        "# Instantiate the model\n",
        "model = Resnet_Unet(BN_enable=True)\n",
        "\n",
        "# Optionally load the model weights if you saved them separately\n",
        "# model.load_state_dict(torch.load('your_model_weights.pth'))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load full dataset function (RGB)\n",
        "def load_full_dataset(train_image_folder, train_mask_folder):\n",
        "    images = []\n",
        "    masks = []\n",
        "\n",
        "    # List all image files in the image folder\n",
        "    image_files = sorted(os.listdir(train_image_folder))\n",
        "    mask_files = sorted(os.listdir(train_mask_folder))\n",
        "\n",
        "    for img_file, mask_file in zip(image_files, mask_files):\n",
        "        # Construct full file path\n",
        "        img_path = os.path.join(train_image_folder, img_file)\n",
        "        mask_path = os.path.join(train_mask_folder, mask_file)\n",
        "\n",
        "        # Load the image as RGB\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)  # Read as RGB\n",
        "        if img is None:\n",
        "            print(f\"Warning: Unable to read image {img_path}\")\n",
        "            continue\n",
        "        img = cv2.resize(img, (256, 256))  # Resize to match the model input\n",
        "\n",
        "        # Load the mask\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # Read as grayscale\n",
        "        if mask is None:\n",
        "            print(f\"Warning: Unable to read mask {mask_path}\")\n",
        "            continue\n",
        "        mask = cv2.resize(mask, (256, 256))  # Resize to match the model input\n",
        "\n",
        "        # Append to lists\n",
        "        images.append(img)\n",
        "        masks.append(mask.reshape((256, 256, 1)))  # Add channel dimension to masks\n",
        "\n",
        "    return np.array(images), np.array(masks)\n",
        "\n",
        "# Specify your folder paths\n",
        "train_image_folder = '/content/drive/MyDrive/iirs/small_data/processed_tiles_of_tiff_delhi_ncr'\n",
        "train_mask_folder = '/content/drive/MyDrive/iirs/small_data/processed_raster_mask_tiles'\n",
        "\n",
        "# Load your entire dataset\n",
        "all_images, all_masks = load_full_dataset(train_image_folder, train_mask_folder)\n",
        "\n",
        "# Ensure the data is in the right shape\n",
        "print(f\"Loaded images shape: {all_images.shape}\")  # Should be (99, 256, 256, 3)\n",
        "print(f\"Loaded masks shape: {all_masks.shape}\")    # Should be (99, 256, 256, 1)\n",
        "\n",
        "# Normalize images if necessary\n",
        "all_images = all_images.astype('float32') / 255.0\n",
        "all_masks = all_masks.astype('float32') / 255.0  # Ensure masks are also in the right format\n",
        "\n",
        "# Check the normalized data shapes\n",
        "print(f\"Normalized images shape: {all_images.shape}\")\n",
        "print(f\"Normalized masks shape: {all_masks.shape}\")\n",
        "\n",
        "# U-Net Model Definition\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, mid_channels, out_channels, upsample_mode='pixelshuffle', BN_enable=True):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.mid_channels = mid_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.upsample_mode = upsample_mode\n",
        "        self.BN_enable = BN_enable\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=mid_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        if self.BN_enable:\n",
        "            self.norm1 = nn.BatchNorm2d(mid_channels)\n",
        "        self.relu1 = nn.ReLU(inplace=False)\n",
        "        self.relu2 = nn.ReLU(inplace=False)\n",
        "\n",
        "        if self.upsample_mode == 'deconv':\n",
        "            self.upsample = nn.ConvTranspose2d(in_channels=mid_channels, out_channels=out_channels,\n",
        "                                                kernel_size=3, stride=2, padding=1, output_padding=1, bias=False)\n",
        "        elif self.upsample_mode == 'pixelshuffle':\n",
        "            self.upsample = nn.PixelShuffle(upscale_factor=2)\n",
        "        if self.BN_enable:\n",
        "            self.norm2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        if self.BN_enable:\n",
        "            x = self.norm1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.upsample(x)\n",
        "        if self.BN_enable:\n",
        "            x = self.norm2(x)\n",
        "        x = self.relu2(x)\n",
        "        return x\n",
        "\n",
        "class Resnet_Unet(nn.Module):\n",
        "    def __init__(self, BN_enable=True, resnet_pretrain=False):\n",
        "        super().__init__()\n",
        "        self.BN_enable = BN_enable\n",
        "        resnet = models.resnet50(pretrained=resnet_pretrain)\n",
        "        filters = [64, 256, 512, 1024, 2048]\n",
        "        self.firstconv = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False)  # Changed to 3 channels\n",
        "        self.firstbn = resnet.bn1\n",
        "        self.firstrelu = resnet.relu\n",
        "        self.firstmaxpool = resnet.maxpool\n",
        "        self.encoder1 = resnet.layer1\n",
        "        self.encoder2 = resnet.layer2\n",
        "        self.encoder3 = resnet.layer3\n",
        "\n",
        "        # Decoder parts\n",
        "        self.center = DecoderBlock(in_channels=filters[3], mid_channels=filters[3]*4, out_channels=filters[3], BN_enable=self.BN_enable)\n",
        "        self.decoder1 = DecoderBlock(in_channels=filters[3]+filters[2], mid_channels=filters[2]*4, out_channels=filters[2], BN_enable=self.BN_enable)\n",
        "        self.decoder2 = DecoderBlock(in_channels=filters[2]+filters[1], mid_channels=filters[1]*4, out_channels=filters[1], BN_enable=self.BN_enable)\n",
        "        self.decoder3 = DecoderBlock(in_channels=filters[1]+filters[0], mid_channels=filters[0]*4, out_channels=filters[0], BN_enable=self.BN_enable)\n",
        "\n",
        "        if self.BN_enable:\n",
        "            self.final = nn.Sequential(\n",
        "                nn.Conv2d(in_channels=filters[0], out_channels=32, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(32),\n",
        "                nn.ReLU(inplace=False),\n",
        "                nn.Conv2d(in_channels=32, out_channels=1, kernel_size=1),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "        else:\n",
        "            self.final = nn.Sequential(\n",
        "                nn.Conv2d(in_channels=filters[0], out_channels=32, kernel_size=3, padding=1),\n",
        "                nn.ReLU(inplace=False),\n",
        "                nn.Conv2d(in_channels=32, out_channels=1, kernel_size=1),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.firstconv(x)\n",
        "        x = self.firstbn(x)\n",
        "        x = self.firstrelu(x)\n",
        "        x_ = self.firstmaxpool(x)\n",
        "\n",
        "        e1 = self.encoder1(x_)\n",
        "        e2 = self.encoder2(e1)\n",
        "        e3 = self.encoder3(e2)\n",
        "\n",
        "        center = self.center(e3)\n",
        "\n",
        "        d2 = self.decoder1(torch.cat([center, e2], dim=1))\n",
        "        d3 = self.decoder2(torch.cat([d2, e1], dim=1))\n",
        "        d4 = self.decoder3(torch.cat([d3, x], dim=1))\n",
        "\n",
        "        return self.final(d4)\n",
        "\n",
        "# Function to calculate accuracy\n",
        "def calculate_accuracy(outputs, masks):\n",
        "    predicted = (outputs > 0.5).float()  # Apply a threshold of 0.5\n",
        "    correct = (predicted == masks).float()  # Compare predicted with true masks\n",
        "    accuracy = correct.sum() / correct.numel()  # Calculate accuracy\n",
        "    return accuracy.item()\n",
        "\n",
        "# Instantiate the model\n",
        "model = Resnet_Unet(BN_enable=True)\n",
        "\n",
        "# Optionally load model weights if saved separately\n",
        "# model.load_state_dict(torch.load('path_to_weights.pth'))\n",
        "\n",
        "# Set up training parameters\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
        "\n",
        "# Prepare dataset for training\n",
        "train_images = torch.from_numpy(all_images).permute(0, 3, 1, 2)  # Change shape to (N, C, H, W)\n",
        "train_masks = torch.from_numpy(all_masks)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_images, val_images, train_masks, val_masks = train_test_split(\n",
        "    train_images, train_masks, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(train_images, train_masks)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    epoch_loss = 0.0\n",
        "    epoch_accuracy = 0.0\n",
        "    total_batches = 0\n",
        "\n",
        "    for imgs, masks in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs.float())  # Forward pass\n",
        "\n",
        "        # Reshape masks to match the output shape (N, C, H, W)\n",
        "        masks = masks.permute(0, 3, 1, 2)  # Reshape\n",
        "\n",
        "        loss = criterion(outputs, masks.float())  # Calculate loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Optimize weights\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = calculate_accuracy(outputs, masks.float())\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_accuracy += accuracy\n",
        "        total_batches += 1\n",
        "\n",
        "    # Average loss and accuracy for the epoch\n",
        "    avg_loss = epoch_loss / total_batches\n",
        "    avg_accuracy = epoch_accuracy / total_batches\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}\")\n",
        "\n",
        "print(\"Training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RitxOVyKqyh",
        "outputId": "58ad744c-b5f7-4fd5-ed85-1f46df71cf25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded images shape: (99, 256, 256, 3)\n",
            "Loaded masks shape: (99, 256, 256, 1)\n",
            "Normalized images shape: (99, 256, 256, 3)\n",
            "Normalized masks shape: (99, 256, 256, 1)\n",
            "Epoch [1/50], Loss: 0.6540, Accuracy: 0.7669\n",
            "Epoch [2/50], Loss: 0.5378, Accuracy: 0.9278\n",
            "Epoch [3/50], Loss: 0.4622, Accuracy: 0.9260\n",
            "Epoch [4/50], Loss: 0.4185, Accuracy: 0.9295\n",
            "Epoch [5/50], Loss: 0.3959, Accuracy: 0.9307\n",
            "Epoch [6/50], Loss: 0.3753, Accuracy: 0.9345\n",
            "Epoch [7/50], Loss: 0.3563, Accuracy: 0.9328\n",
            "Epoch [8/50], Loss: 0.3419, Accuracy: 0.9327\n",
            "Epoch [9/50], Loss: 0.3271, Accuracy: 0.9337\n",
            "Epoch [10/50], Loss: 0.3168, Accuracy: 0.9343\n",
            "Epoch [11/50], Loss: 0.3044, Accuracy: 0.9338\n",
            "Epoch [12/50], Loss: 0.2936, Accuracy: 0.9357\n",
            "Epoch [13/50], Loss: 0.2833, Accuracy: 0.9336\n",
            "Epoch [14/50], Loss: 0.2760, Accuracy: 0.9330\n",
            "Epoch [15/50], Loss: 0.2677, Accuracy: 0.9333\n",
            "Epoch [16/50], Loss: 0.2583, Accuracy: 0.9367\n",
            "Epoch [17/50], Loss: 0.2492, Accuracy: 0.9351\n",
            "Epoch [18/50], Loss: 0.2410, Accuracy: 0.9325\n",
            "Epoch [19/50], Loss: 0.2324, Accuracy: 0.9328\n",
            "Epoch [20/50], Loss: 0.2252, Accuracy: 0.9315\n",
            "Epoch [21/50], Loss: 0.2177, Accuracy: 0.9340\n",
            "Epoch [22/50], Loss: 0.2101, Accuracy: 0.9370\n",
            "Epoch [23/50], Loss: 0.2036, Accuracy: 0.9355\n",
            "Epoch [24/50], Loss: 0.1972, Accuracy: 0.9324\n",
            "Epoch [25/50], Loss: 0.1907, Accuracy: 0.9345\n",
            "Epoch [26/50], Loss: 0.1846, Accuracy: 0.9340\n",
            "Epoch [27/50], Loss: 0.1786, Accuracy: 0.9348\n",
            "Epoch [28/50], Loss: 0.1727, Accuracy: 0.9354\n",
            "Epoch [29/50], Loss: 0.1675, Accuracy: 0.9336\n",
            "Epoch [30/50], Loss: 0.1621, Accuracy: 0.9341\n",
            "Epoch [31/50], Loss: 0.1567, Accuracy: 0.9335\n",
            "Epoch [32/50], Loss: 0.1510, Accuracy: 0.9340\n",
            "Epoch [33/50], Loss: 0.1466, Accuracy: 0.9348\n",
            "Epoch [34/50], Loss: 0.1416, Accuracy: 0.9341\n",
            "Epoch [35/50], Loss: 0.1370, Accuracy: 0.9351\n",
            "Epoch [36/50], Loss: 0.1325, Accuracy: 0.9344\n",
            "Epoch [37/50], Loss: 0.1284, Accuracy: 0.9360\n",
            "Epoch [38/50], Loss: 0.1240, Accuracy: 0.9311\n",
            "Epoch [39/50], Loss: 0.1202, Accuracy: 0.9356\n",
            "Epoch [40/50], Loss: 0.1162, Accuracy: 0.9354\n",
            "Epoch [41/50], Loss: 0.1123, Accuracy: 0.9334\n",
            "Epoch [42/50], Loss: 0.1086, Accuracy: 0.9375\n",
            "Epoch [43/50], Loss: 0.1049, Accuracy: 0.9343\n",
            "Epoch [44/50], Loss: 0.1018, Accuracy: 0.9352\n",
            "Epoch [45/50], Loss: 0.0981, Accuracy: 0.9371\n",
            "Epoch [46/50], Loss: 0.0952, Accuracy: 0.9345\n",
            "Epoch [47/50], Loss: 0.0922, Accuracy: 0.9345\n",
            "Epoch [48/50], Loss: 0.0893, Accuracy: 0.9357\n",
            "Epoch [49/50], Loss: 0.0865, Accuracy: 0.9327\n",
            "Epoch [50/50], Loss: 0.0837, Accuracy: 0.9345\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GIVING ONLY LOSS(GOOD FOR JUGAD)"
      ],
      "metadata": {
        "id": "nY2f_3vFMZZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load full dataset function (RGB)\n",
        "def load_full_dataset(train_image_folder, train_mask_folder):\n",
        "    images = []\n",
        "    masks = []\n",
        "\n",
        "    # List all image files in the image folder\n",
        "    image_files = sorted(os.listdir(train_image_folder))\n",
        "    mask_files = sorted(os.listdir(train_mask_folder))\n",
        "\n",
        "    for img_file, mask_file in zip(image_files, mask_files):\n",
        "        # Construct full file path\n",
        "        img_path = os.path.join(train_image_folder, img_file)\n",
        "        mask_path = os.path.join(train_mask_folder, mask_file)\n",
        "\n",
        "        # Load the image as RGB\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)  # Read as RGB\n",
        "        if img is None:\n",
        "            print(f\"Warning: Unable to read image {img_path}\")\n",
        "            continue\n",
        "        img = cv2.resize(img, (256, 256))  # Resize to match the model input\n",
        "\n",
        "        # Load the mask\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # Read as grayscale\n",
        "        if mask is None:\n",
        "            print(f\"Warning: Unable to read mask {mask_path}\")\n",
        "            continue\n",
        "        mask = cv2.resize(mask, (256, 256))  # Resize to match the model input\n",
        "\n",
        "        # Append to lists\n",
        "        images.append(img)\n",
        "        masks.append(mask.reshape((256, 256, 1)))  # Add channel dimension to masks\n",
        "\n",
        "    return np.array(images), np.array(masks)\n",
        "\n",
        "# Specify your folder paths\n",
        "train_image_folder = '/content/drive/MyDrive/iirs/small_data/processed_tiles_of_tiff_delhi_ncr'\n",
        "train_mask_folder = '/content/drive/MyDrive/iirs/small_data/processed_raster_mask_tiles'\n",
        "\n",
        "# Load your entire dataset\n",
        "all_images, all_masks = load_full_dataset(train_image_folder, train_mask_folder)\n",
        "\n",
        "# Ensure the data is in the right shape\n",
        "print(f\"Loaded images shape: {all_images.shape}\")  # Should be (99, 256, 256, 3)\n",
        "print(f\"Loaded masks shape: {all_masks.shape}\")    # Should be (99, 256, 256, 1)\n",
        "\n",
        "# Normalize images if necessary\n",
        "all_images = all_images.astype('float32') / 255.0\n",
        "all_masks = all_masks.astype('float32') / 255.0  # Ensure masks are also in the right format\n",
        "\n",
        "# Check the normalized data shapes\n",
        "print(f\"Normalized images shape: {all_images.shape}\")\n",
        "print(f\"Normalized masks shape: {all_masks.shape}\")\n",
        "\n",
        "# U-Net Model Definition\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, mid_channels, out_channels, upsample_mode='pixelshuffle', BN_enable=True):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.mid_channels = mid_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.upsample_mode = upsample_mode\n",
        "        self.BN_enable = BN_enable\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=mid_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        if self.BN_enable:\n",
        "            self.norm1 = nn.BatchNorm2d(mid_channels)\n",
        "        self.relu1 = nn.ReLU(inplace=False)\n",
        "        self.relu2 = nn.ReLU(inplace=False)\n",
        "\n",
        "        if self.upsample_mode == 'deconv':\n",
        "            self.upsample = nn.ConvTranspose2d(in_channels=mid_channels, out_channels=out_channels,\n",
        "                                                kernel_size=3, stride=2, padding=1, output_padding=1, bias=False)\n",
        "        elif self.upsample_mode == 'pixelshuffle':\n",
        "            self.upsample = nn.PixelShuffle(upscale_factor=2)\n",
        "        if self.BN_enable:\n",
        "            self.norm2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        if self.BN_enable:\n",
        "            x = self.norm1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.upsample(x)\n",
        "        if self.BN_enable:\n",
        "            x = self.norm2(x)\n",
        "        x = self.relu2(x)\n",
        "        return x\n",
        "\n",
        "class Resnet_Unet(nn.Module):\n",
        "    def __init__(self, BN_enable=True, resnet_pretrain=False):\n",
        "        super().__init__()\n",
        "        self.BN_enable = BN_enable\n",
        "        resnet = models.resnet50(pretrained=resnet_pretrain)\n",
        "        filters = [64, 256, 512, 1024, 2048]\n",
        "        self.firstconv = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False)  # Changed to 3 channels\n",
        "        self.firstbn = resnet.bn1\n",
        "        self.firstrelu = resnet.relu\n",
        "        self.firstmaxpool = resnet.maxpool\n",
        "        self.encoder1 = resnet.layer1\n",
        "        self.encoder2 = resnet.layer2\n",
        "        self.encoder3 = resnet.layer3\n",
        "\n",
        "        # Decoder parts\n",
        "        self.center = DecoderBlock(in_channels=filters[3], mid_channels=filters[3]*4, out_channels=filters[3], BN_enable=self.BN_enable)\n",
        "        self.decoder1 = DecoderBlock(in_channels=filters[3]+filters[2], mid_channels=filters[2]*4, out_channels=filters[2], BN_enable=self.BN_enable)\n",
        "        self.decoder2 = DecoderBlock(in_channels=filters[2]+filters[1], mid_channels=filters[1]*4, out_channels=filters[1], BN_enable=self.BN_enable)\n",
        "        self.decoder3 = DecoderBlock(in_channels=filters[1]+filters[0], mid_channels=filters[0]*4, out_channels=filters[0], BN_enable=self.BN_enable)\n",
        "\n",
        "        if self.BN_enable:\n",
        "            self.final = nn.Sequential(\n",
        "                nn.Conv2d(in_channels=filters[0], out_channels=32, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(32),\n",
        "                nn.ReLU(inplace=False),\n",
        "                nn.Conv2d(in_channels=32, out_channels=1, kernel_size=1),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "        else:\n",
        "            self.final = nn.Sequential(\n",
        "                nn.Conv2d(in_channels=filters[0], out_channels=32, kernel_size=3, padding=1),\n",
        "                nn.ReLU(inplace=False),\n",
        "                nn.Conv2d(in_channels=32, out_channels=1, kernel_size=1),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.firstconv(x)\n",
        "        x = self.firstbn(x)\n",
        "        x = self.firstrelu(x)\n",
        "        x_ = self.firstmaxpool(x)\n",
        "\n",
        "        e1 = self.encoder1(x_)\n",
        "        e2 = self.encoder2(e1)\n",
        "        e3 = self.encoder3(e2)\n",
        "\n",
        "        center = self.center(e3)\n",
        "\n",
        "        d2 = self.decoder1(torch.cat([center, e2], dim=1))\n",
        "        d3 = self.decoder2(torch.cat([d2, e1], dim=1))\n",
        "        d4 = self.decoder3(torch.cat([d3, x], dim=1))\n",
        "\n",
        "        return self.final(d4)\n",
        "\n",
        "# Instantiate the model\n",
        "model = Resnet_Unet(BN_enable=True)\n",
        "\n",
        "# Optionally load model weights if saved separately\n",
        "# model.load_state_dict(torch.load('path_to_weights.pth'))\n",
        "\n",
        "# Set up training parameters\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
        "\n",
        "# Prepare dataset for training\n",
        "train_images = torch.from_numpy(all_images).permute(0, 3, 1, 2)  # Change shape to (N, C, H, W)\n",
        "train_masks = torch.from_numpy(all_masks)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_images, val_images, train_masks, val_masks = train_test_split(\n",
        "    train_images, train_masks, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(train_images, train_masks)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    for imgs, masks in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs.float())  # Forward pass\n",
        "        # Reshape masks to match the output shape (N, C, H, W)\n",
        "        masks = masks.permute(0, 3, 1, 2)  # Reshape\n",
        "        loss = criterion(outputs, masks.float())  # Calculate loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Optimize weights\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "print(\"Training complete.\")\n"
      ],
      "metadata": {
        "id": "c7GOz4YAMTmI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}